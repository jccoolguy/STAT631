---
title: "STAT 631 Homework 3"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 09/16/2024
date-format: short
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsmath}
editor: visual
engine: knitr
---

1\)

```{r fig.width=10, fig.height=10}
 library(MASS) # need for mvrnorm
par(mfrow=c(2,2))
N = 2500
nu = 3
set.seed(5640)
cov=matrix(c(1, 0.8, 0.8, 1), nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w, w)
plot(x, main = "(a)")
 
set.seed(5640)
cov=matrix(c(1, 0.8, 0.8, 1),nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w1 = sqrt(nu / rchisq(N, df = nu))
w2 = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w1, w2)
plot(x, main = "(b)")
 
set.seed(5640)
cov=matrix(c(1, 0, 0, 1), nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w1 = sqrt(nu / rchisq(N, df = nu))
w2 = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w1, w2)
plot(x, main = "(c)")

set.seed(5640)
cov=matrix(c(1, 0, 0, 1), nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w, w)
plot(x, main = "(d)")

```

Problem 3)

The sample with independent variates is c), we can see the density is circular. We can also see that there is no tail dependence, extreme values in $x[,1]$ are not correlated with extreme values in $x[,2]$ and vice versa.

Also if we look at the code used, the following covariance matrix $\Sigma$ is used in the simulation of the multivariate normal data.

$$
\Sigma=\begin{bmatrix} 1 &0 \\0& 1\end{bmatrix}
$$

Since $\Sigma$ is diagonal we know components $x_1,x_2$ are independent.

We know there is no tail dependence because when we generate the marginal t-distributions using $x_1,x_2$ the random chi-square component is different for $w_1,w_2$. There is no shared component in the marginal t-distributions.

Problem 4)

The sample with correlated variates that do not have tail dependence is b). We can see the correlation from the shape in the center of the density, it is diagonal stretching both to the top right and the bottom left.

If we look at the code we can see the following $\Sigma$ matrix when generating the multivariate normal data:

$$
\Sigma= \begin{bmatrix} 1 &0.8 \\ 0.8 &1\end{bmatrix}
$$

We can see the covariance in the non diagonal entries.

We know there is no tail dependence because we generate different chi-square components for each marginal t distribution.

Problem 5)

The sample with uncorrelated variates with tail dependence is d. We can see the density shape in the middle is circular but we also see that extreme values in $x[,1]$ are also often extreme in $x[,2]$ and vice versa.

If we look at the code we can see the following $\Sigma$ matrix when generating the multivariate normal data:

$$
\Sigma=\begin{bmatrix} 1 & 0 \\ 0 &1\end{bmatrix}
$$

Since $\Sigma$ is diagonal, the variates are independent.

We know there is tail dependence because each marginal t distribution in the multivariate t -distribution share the same chi-square component.

Problem 6)

a\)

We know the below:

$$
\nu=5,\mu=\begin{bmatrix} 0.001 \\ 0.002\end{bmatrix},\Sigma=\begin{bmatrix} 0.10 & 0.03 \\ 0.03 & 0.15\end{bmatrix}
$$

We use the relationship $\Sigma=\frac{v}{v-2}\Lambda$. Then:

$$
\Lambda=\frac{3}{5}\begin{bmatrix}0.1 &0.03 \\ 0.03 & 0.15 \end{bmatrix}
$$

So then the multivariate t-distribution of $(X,Y)$, which we can label as S is:

$$
S \sim t_{5}(\mu,\Lambda)
$$ Since R is a portfolio of the two stocks and they are evenly weighted we can have weight vector $w=[0.5,0.5]^T$. Then, since $S$ is a multivariate t-distribution we can say:

$$
R=w^TS\sim t_{\nu}(w^T\mu,w^T \Lambda w)
$$

```{r}
w <- c(.5,.5)
Lambda <- matrix(c(.1, 0.03, 0.03, 0.15), nrow = 2, ncol = 2)
mu <- c(0.001,0.002)
R_mu <- t(w)%*%mu
R_lambda <- t(w)%*%Lambda%*%w

c(R_mu,R_lambda)
```

$$
R\sim t_{5}(0.0015,0.0775)
$$

b\)

```{r}
set.seed(200128)
x <- 0.0015 + rt(10000, df = 5)*sqrt(0.0775)
quantile_99 <- quantile(x, probs = .99)
avg_greater <- mean(x[which(x > quantile_99)])
results <- c(quantile_99,avg_greater)
names(results) <- c("99th Quantile","Sample Avg > 99th q")
results
```

2\)

```{r message=FALSE}
library(quantmod)
syb = c("GIS","KDP","KO","PG")
d = length(syb)
rt = c()

for(i in 1:d){
  getSymbols(syb[i], from = "2011-01-01", to = "2024-08-31")
  rt = cbind(rt, weeklyReturn(Ad(get(syb[i])), type = "log")[-1])
}
colnames(rt) = syb
rt = rt*100
```

a\)

We can create a scatter-plot matrix for these returns.

```{r fig.height=12, fig.width=12}
par(mfrow = c(4,4))
pairs(~GIS + KDP + KO + PG, data = rt)
```

The first step of estimating the variance of $.5X_1+.3X_2+.2X_3$ is getting the sample covariance matrix

```{r}
Sigma <- cov(rt[,1:3])
Sigma
```

Then we will use the fact that $Var(w^T Y)=w^TCov(Y)w$, where w is the vector of weights $w=[.5,.3,.2]^T$.

```{r}
w = c(.5,.3,.2)
variance_est <- t(w) %*% Sigma %*% w
variance_est
```

The estimated variance is `r variance_est`.

b\)

The multivariate t distribution requires all marginal distributions to have the same degrees of freedom. We must check that assumption.

```{r}
library(MASS)
nu = c(); se = c();
for(i in 1:4){
  start = list(m = mean(rt[,i]), s = sd(rt[,i]), df = 4)
  fit = fitdistr(rt[,i],"t", start, lower = c(-100, 0.0001, 0.1))
  nu[i] = fit$est["df"]
  se[i] = fit$sd["df"]
}
stat = cbind(nu,se,nu - qnorm(.975)*se, nu + qnorm(.975)*se)
rownames(stat) = syb
colnames(stat) = c("nu","std err","lower 95%", "upper 95%")
stat
```

Since each fitted marginal t distribution appears to have similar degrees of freedom the multivariate t distribution is a suitable candidate model for rt. Some other reasons we would use this model are as follows. The tails of return data are often longer and fatter than the normal distribution. Additionally in our scatter plot we can see the tail dependence in the pairs of return data. Using the multivariate t distribution also has convenient properties for analyzing a portfolio, it can be shown that given a weight w and a multivariate t distribution of returns that the distribution of the portfolio's return is $t_\nu(w^T\mu,w^T\Lambda w)$. Where $\mu$ and $\Lambda$ are the location vector and scale matrix respectively.

c\)

```{r}
library(mnormt)
df = seq(2.7, 5.7, 0.01)
n = length(df)
loglik_profile = rep(0,n)
for(i in 1:n){
  fit = cov.trob(rt, nu = df[i])
  mu = as.vector(fit$center)
  sigma = matrix(fit$cov, nrow = 4)
  loglik_profile[i] = sum(log(dmt(rt, mean = fit$center,
                                  S = fit$cov, df = df[i])))
}
```

To find the MLE of $\nu$ we find where profile likelihood is maximized.

```{r}
df_mle <- df[which.max(loglik_profile)]
df_mle
```

The MLE of $\nu$ is `r df_mle`.

We accept the null hypothesis $H_0:\theta_1=\theta_{0,1}$ if $L_p(\theta_{0,1})>L_P(\hat{\theta}_1)-\frac{1}{2}\chi^2_{\alpha,1}$. Where $\hat{\theta_1}$ is the MLE of $\nu$ that we previously computed. Then our 90% confidence interval is the range of $\nu$ that fail to reject the null hypothesis for each hypothesized value of $\nu$.

```{r}
critical_value <- max(loglik_profile) - 1/2*qchisq(.90,1)
lower <- min(df[which(loglik_profile > critical_value)])
upper <- max(df[which(loglik_profile > critical_value)])

conf_interval <- c(lower,upper)
names(conf_interval) <- c("lower.90%","upper.90%")
conf_interval
```

The plot of the log-likelihood marking its MLE and confidence interval is below:

```{r}
plot(x = df, y = loglik_profile, main = "Profile Likelihood", xlab = "DF", type = "l")
abline(v = conf_interval[1])
abline(v = conf_interval[2])
points(x = df_mle, y = max(loglik_profile), col = "red")
```

d\)

AIC and BIC are defined as:

$$
\text{AIC}=-2\log(\hat{\nu}_{ML})+2p \\ \text{BIC}=-2log(\hat{\nu}_{ML})+p\log(n)
$$

```{r}
d = dim(rt)[2]; n = dim(rt)[1]
p = d*(d+1)/2 + d + 1 

AIC <- -2*max(loglik_profile) + 2*p
BIC <- -2*max(loglik_profile) + p*log(length(n))
information_criteria <- c(AIC,BIC)
names(information_criteria) <- c("AIC","BIC")
information_criteria
```

e\)

```{r message=FALSE}
library(sn)
```

```{r}
fit_st = mst.mple(y = rt)
estimated_nu <- fit_st$dp$nu
estimated_nu
dp2cp(fit_st$dp, "st")

```

Our estimated $\nu$ is greater than 4, so we use the dp2cp() function to obtain the mean vector, covariance matrix and skewness.

f\)

The addition of the skewness parameter increases our p from 15 to 19.

```{r}
p_st = d*(d+1)/2 + d + 1 + d

aic_st = -2*fit_st$logL + 2*p_st
bic_st = -2*fit_st$logL + p_st*log(n)

ic_st <- c(aic_st, bic_st)
names(ic_st) <- c("AIC","BIC")
ic_st
```

We can compare the information criteria for the multivariate t distribution and the skewed multivariate t distribution.

```{r}
ic_compare <- rbind(information_criteria, ic_st)
rownames(ic_compare) = c("Multivariate T","Multivariate Skewed T")
ic_compare
```

The AIC would choose the multivariate skewed T distribution and BIC would choose the multivariate T distribution.

3\)

```{r}
load("HW03.Rdata")
```

```{r}
head(xt,3)
```

a\)

We want to check that the degrees of freedom for each fitted marginal t distribution are similar to each other.

```{r}
univ.t$mle[,"df"]
```

```{r}
univ.t$se[,"df"]
```

We can create a confidence interval for each fitted marginal t distributions' degree of freedom.

```{r}
upper_df <- univ.t$mle[,"df"] + 1.96*univ.t$se[,"df"]
lower_df <- univ.t$mle[,"df"] - 1.96*univ.t$se[,"df"]
conf_dfs <- rbind(lower_df,upper_df)
rownames(conf_dfs) <- c("Lower.95%","Upper.95%")
conf_dfs
```

A multivariate-t model is inappropriate for xt, the degrees of freedom for each marginal t distribution are not identical. Specifically AXP returns' fitted degrees of freedom is far smaller than the other three returns.

b\)

We can model ACM, CVX and JNJ with a multivariate-t model since their marginal t distributions have similar degrees of freedom.

```{r}
xt_mvt <- xt[,c(1,3,4)]
```

```{r}
df = seq(3.2, 4.9, 0.01)
n = length(df)
loglik_profile = rep(0,n)
for(i in 1:n){
  fit = cov.trob(xt_mvt, nu = df[i])
  mu = as.vector(fit$center)
  sigma = matrix(fit$cov, nrow = 3)
  loglik_profile[i] = sum(log(dmt(xt_mvt, mean = fit$center,
                                  S = fit$cov, df = df[i])))
}
```

```{r}
df_xt_mvt <- df[which.max(loglik_profile)]
df_xt_mvt
```

The degrees of freedom that maximizes the likelihood is `r df_xt_mvt`. Now we can fit the model.

```{r}
est = cov.trob(xt_mvt, nu = df_xt_mvt, cor = T)
names(est)
```

c\)

```{r}
est$center
diag(est$cov)
```

The marginal distribution for each return is as follows: ACM is $t_5(0.04700037,1.9843306)$, CVX is $t_5(0.05358264,1.0402260)$ and JNJ is $t_5(0.06739520,0.4834828)$.

d\)

The hypothesis we are testing is $H_0:\nu=4,H_a:\nu \neq4$. Our test statistic is $2(\log L(\hat{\nu}_{ML})-\log L(\hat{\nu}_{0,ML})) \geq\chi_{\alpha,1}$.

```{r}
log_lik_at_mle <- max(loglik_profile)
log_lik_at_4 <- loglik_profile[which(df==4)]
test_statistic <- 2*(log_lik_at_mle - log_lik_at_4)
p_value <- 1 - pchisq(test_statistic, df = 1)
results <- c(test_statistic, p_value)
names(results) <- c("Test Statistic", "P-Value")
results
```

With a P-value of `r round(p_value,3)` at a significance level of .05 we cannot reject the null hypothesis of $\nu$ being equal to 4.
