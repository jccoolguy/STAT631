---
title: "STAT 631 Homework 3"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 09/16/2024
date-format: short
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsmath}
editor: visual
engine: knitr
---

1\)

```{r}
 library(MASS) # need for mvrnorm
par(mfrow=c(1,4))
N = 2500
nu = 3
set.seed(5640)
cov=matrix(c(1, 0.8, 0.8, 1), nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w, w)
plot(x, main = "(a)")
 
set.seed(5640)
cov=matrix(c(1, 0.8, 0.8, 1),nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w1 = sqrt(nu / rchisq(N, df = nu))
w2 = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w1, w2)
plot(x, main = "(b)")
 
set.seed(5640)
cov=matrix(c(1, 0, 0, 1), nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w1 = sqrt(nu / rchisq(N, df = nu))
w2 = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w1, w2)
plot(x, main = "(c)")

set.seed(5640)
cov=matrix(c(1, 0, 0, 1), nrow = 2)
x= mvrnorm(N, mu = c(0, 0), Sigma = cov)
w = sqrt(nu / rchisq(N, df = nu))
x=x*cbind(w, w)
plot(x, main = "(d)")

```

Problem 3)

The sample with independent variates is c), we can see the density is circular. We can also see that there is no tail dependence, extreme in values in $x[,1]$ are not correlated with extreme values in $x[,2]$ and vice versa.

Also if we look at the code we use the following $\Sigma$ matrix when generating the multivariate normal data:

$$
\Sigma=\begin{bmatrix} 1 &0 \\0& 1\end{bmatrix}
$$

Since $\Sigma$ is diagonal we know components $x_1,x_2$ are independent.

We know there is no tail dependence because when we generate the two multivariate t -distribution we randomly pull from two different chi-square distributions.

Problem 4)

The sample with correlated variates that do not have tail dependence is b). We can see the correlation from the shape in the center of the density, it is diagonal stretching both to the top right and the bottom left.

If we look at the code we can see the following $\Sigma$ matrix when generating the multivariate normal data:

$$
\Sigma= \begin{bmatrix} 1 &0.8 \\ 0.8 &1\end{bmatrix}
$$

We can see the covariance in the non diagonal entries.

We know there is no tail dependence because when we generate the two multivariate t -distribution we randomly pull from two different chi-square distributions.

Problem 5)

The sample with uncorrelated variates with tail dependence is b). We can see the density shape in the middle is circular but we also see that extreme values in $x[,1]$ are also often extreme in $x[,2]$ and vice versa.

If we look at the code we can see the following $\Sigma$ matrix when generating the multivariate normal data:

$$
\Sigma=\begin{bmatrix} 1 & 0 \\ 0 &1\end{bmatrix}
$$

Since $\Sigma$ is diagonal, the variates are independent.

We know there is tail dependence because the two multivariate t -distributions that are generated are identical to each other, only one random pull from the $\chi^2$ distribution is performed.

Problem 6)

a\)

We know the below:

$$
\nu=5,\mu=\begin{bmatrix} 0.001 \\ 0.002\end{bmatrix},\Sigma=\begin{bmatrix} 0.10 & 0.03 \\ 0.03 & 0.15\end{bmatrix}
$$

We use the relationship $\Sigma=\frac{v}{v-2}\Lambda$. Then:

$$
\Lambda=\frac{3}{5}\begin{bmatrix}0.1 &0.03 \\ 0.03 & 0.15 \end{bmatrix}
$$

So then the multivariate t-distribution of $(X,Y)$, which we can label as S is:

$$
S \sim t_{5}(\mu,\Lambda)
$$ Since R is a portfolio of the two stocks and they are evenly weighted we can have weight vector $w=[0.5,0.5]^T$. Then, since $S$ is a multivariate t-distribution we can say:

$$
R=w^TS\sim t_{\nu}(w^T\mu,w^T \Lambda w)
$$

```{r}
w <- c(.5,.5)
Lambda <- matrix(c(.1, 0.03, 0.03, 0.15), nrow = 2, ncol = 2)
mu <- c(0.001,0.002)
R_mu <- t(w)%*%mu
R_lambda <- t(w)%*%Lambda%*%w

c(R_mu,R_lambda)
```

$$
R\sim t_{5}(0.0015,0.0775)
$$

b\)

```{r}
set.seed(200128)
x <- 0.0015 + rt(10000, df = 5)/0.0775
quantile_99 <- quantile(x, probs = .99)
avg_greater <- mean(x[which(x > quantile_99)])
c(quantile_99,avg_greater)
```

2\)

```{r}
library(quantmod)
syb = c("GIS","KDP","KO","PG")
d = length(syb)
rt = c()

for(i in 1:d){
  getSymbols(syb[i], from = "2011-01-01", to = "2024-08-31")
  rt = cbind(rt, weeklyReturn(Ad(get(syb[i])), type = "log")[-1])
}
colnames(rt) = syb
rt = rt*100
```

a\)

We can create a scatter-plot matrix for these returns.

```{r fig.height= 12, fig.width=12}
par(mfrow = c(4,4))
for (i in (1:4)){
  for (j in (1:4)){
    plot(x = as.numeric(rt[,i]),y = as.numeric(rt[,j]),main = paste(names(rt)[i],"by",names(rt)[j]),xlab = names(rt)[i],ylab = names(rt)[j])
  }
}
```

The first step of estimating the variance of $.5X_1+.3X_2+.2X_3$ we will create the sample covariance matrix for $X_1,X_2,X_3$.

```{r}
covariances <- c()
for(i in 1:3){
  for(j in 1:3){
    covariances <- c(covariances, cov(rt[,i],rt[,j]))
  }
}
Sigma <- matrix(covariances, nrow  = 3, ncol = 3)
Sigma

```

Then we will use the fact that $w^T\Sigma w$ is the variance of the uni-variate t distribution for a portfolio with weights w. In this case $w=\begin{bmatrix} 0.5,0.3,0.2\end{bmatrix}^T$.

```{r}
w = c(.5,.3,.2)
variance_est <- t(w) %*% Sigma %*% w
variance_est
```

The estimated variance is `r variance_est`.

b\)

The multivariate t distribution is a suitable candidate model for rt because the tails of return data are often longer and fatter than the normal distribution. Additionally in our scatter plot we can see the tail dependence in the pairs of return data. Using the multivariate t distribution also has convenient properties for analyzing a portfolio, it can be shown that given a weight w and a multivariate t distribution of returns that the distribution of the portfolio's return is $t_\nu(w^T\mu,w^T\Lambda w)$. Where $\mu$ and $\Lambda$ are the location vector and scale matrix respectively.

c\)

```{r}
library(MASS)
library(mnormt)
df = seq(2.5, 8, 0.01)
n = length(df)
loglik_profile = rep(0,n)
for(i in 1:n){
  fit = cov.trob(rt, nu = df[i])
  mu = as.vector(fit$center)
  sigma = matrix(fit$cov, nrow = 4)
  loglik_profile[i] = sum(log(dmt(rt, mean = fit$center,
                                  S = fit$cov, df = df[i])))
}
```

To find the MLE of $\nu$ which degrees of freedom log-likelihood is maximized.

```{r}
df_mle <- df[which.max(loglik_profile)]
df_mle
```

The MLE of $\nu$ is `r df_mle`.

We accept the null hypothesis $H_0:\theta_1=\theta_{0,1}$ if $L_p(\theta_{0,1})>L_P(\hat{\theta}_1)-\frac{1}{2}\chi^2_{\alpha,1}$. Where $\hat{\theta_1}$ is the MLE of $\nu$ that we previously computed. Then our 90% confidence interval is the range of $\nu$ that fail to reject the null hypothesis for each hypothesized value of $\nu$.

```{r}
critical_value <- max(loglik_profile) - 1/2*qchisq(.90,1)
lower <- min(df[which(loglik_profile > critical_value)])
upper <- max(df[which(loglik_profile > critical_value)])

conf_interval <- c(lower,upper)
names(conf_interval) <- c("lower.90%","upper.90%")
conf_interval
```

d\)

AIC and BIC are defined as:

$$
\text{AIC}=-2\log(\hat{\nu}_{ML})+2p \\ \text{BIC}=-2log(\hat{\nu}_{ML})+p\log(n)
$$

```{r}
d = dim(rt)[2]; n = dim(rt)[1]
p = d*(d+1)/2 + d + 1 

AIC <- -2*max(loglik_profile) + 2*p
BIC <- -2*max(loglik_profile) + p*log(length(n))
information_criteria <- c(AIC,BIC)
names(information_criteria) <- c("AIC","BIC")
information_criteria
```

e\)

```{r}
library(sn)
fit_st = mst.mple(y = rt)
names(fit_st)
fit_st$dp

```

Our estimated $\nu$ is greater than 4, so we use the dp2cp() function to obtain the mean vector, covariance matrix and skewness.

```{r}
dp2cp(fit_st$dp, "st")

```

f\)

The addition of the skewness parameter increases our p from 15 to 19.

```{r}
p_st = d*(d+1)/2 + d + 1 + d

aic_st = -2*fit_st$logL + 2*p_st
bic_st = -2*fit_st$logL + p_st*log(n)

ic_st <- c(aic_st, bic_st)
names(ic_st) <- c("AIC","BIC")
ic_st
```

We can compare the information criteria for the multivariate t distribution and the skewed multivariate t distribution.

```{r}
ic_compare <- rbind(information_criteria, ic_st)
rownames(ic_compare) = c("Multivariate T","Multivariate Skewed T")
ic_compare
```

The AIC would choose the multivariate skewed T distribution and BIC would choose the multivariate T distribution.

3\)

```{r}
load("HW03.Rdata")
```

```{r}
head(xt,3)
```

a\)

We want to check that the degrees of freedom for each fitted marginal t distribution are similar to each other.

```{r}
univ.t$mle[,"df"]
```

```{r}
univ.t$se[,"df"]
```

We can create a confidence interval for each fitted marginal t distributions' degree of freedom.

```{r}
upper_df <- univ.t$mle[,"df"] + 1.96*univ.t$se[,"df"]
lower_df <- univ.t$mle[,"df"] - 1.96*univ.t$se[,"df"]
conf_dfs <- rbind(lower_df,upper_df)
rownames(conf_dfs) <- c("Lower.95%","Upper.95%")
conf_dfs
```

A multivariate-t model is inappropriate for xt, the degrees of freedom for each marginal t distribution are not identical. Specifically AXP returns' fitted degrees of freedom is far smaller than the other three returns.

b\)

We can model ACM, CVX and JNJ with a multivariate-t model since their marginal t distributions have similar degrees of freedom.

```{r}
xt_mvt <- xt[,c(1,3,4)]
```

```{r}
df = seq(2.5, 8, 0.01)
n = length(df)
loglik_profile = rep(0,n)
for(i in 1:n){
  fit = cov.trob(xt_mvt, nu = df[i])
  mu = as.vector(fit$center)
  sigma = matrix(fit$cov, nrow = 3)
  loglik_profile[i] = sum(log(dmt(xt_mvt, mean = fit$center,
                                  S = fit$cov, df = df[i])))
}
```

```{r}
df_xt_mvt <- df[which.max(loglik_profile)]
df_xt_mvt
```

The degrees of freedom that maximizes the likelihood is `r df_xt_mvt`. Now we can fit the model.

```{r}
est = cov.trob(xt_mvt, nu = df_xt_mvt, cor = T)
names(est)
```

c\)

```{r}
est$center
diag(est$cov)
```

The marginal distribution for each return is as follows: ACM is $t_5(0.04700037,1.9843306)$, CVX is $t_5(0.05358264,1.0402260)$ and JNJ is $t_5(0.06739520,0.4834828)$.

d\)

The hypothesis we are testing is $H_0:\nu=4,H_a:\nu \neq4$. Our test statistic is $2(\log L(\hat{\nu}_{ML})-\log L(\hat{\nu}_{0,ML})) \geq\chi_{\alpha,1}$.

```{r}
log_lik_at_mle <- max(loglik_profile)
log_lik_at_4 <- loglik_profile[which(df==4)]
test_statistic <- 2*(log_lik_at_mle - log_lik_at_4)
p_value <- 1 - pchisq(test_statistic, df = 1)
results <- c(test_statistic, p_value)
names(results) <- c("Test Statistic", "P-Value")
results
```

With a P-value of `r round(p_value,3)` at a significance level of .05 we cannot reject the null hypothesis of $\nu$ being equal to 4.
