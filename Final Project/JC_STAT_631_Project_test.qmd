---
title: "STAT 631 Project"
author: "Jack Cunningham (jgavc@tamu.edu)"
date: 12/10/2024
date-format: short
format:
  pdf:
    include-in-header:
      - text: |
          \usepackage{amsmath}
editor: visual
engine: knitr
---

The stock I chose for this project is the Bank of New York Mellon, ticker symbol BK.

Loading data, helper functions and libraries:

```{r, warning=FALSE, message=FALSE}
library(quantmod)
library(forecast)
library(rugarch)
load("garch.RData")
source("GARCH_plotFunctions.R")
source("GARCH_RFunctions.R")
```

# Introduction and Stock Performance

The Bank of New York Mellon is a financial services company. It came to be after the Bank of New York, the oldest bank in the US and the largest custodian bank, and Mellon Financial Corporation, known for asset management, merged in 2007. It has many offerings and is less reliant on deal flow than an investment bank (say Goldman Sachs) and less reliant on interest rates than a consumer bank (say Citibank). A large portion of operations are providing technology solutions to asset managers and other banks.

Let's take a look at BK's performance over time:

```{r}
plot(BK[,"BK.Adjusted"])
```

There are a few interesting events to point out.

First, the time series begins at the start of 2009 where the financial industry was working its way out of the great financial crisis, we see the stock struggle until the beginning of 2012 where it has a strong rally for a couple of years.

Second, in 2015 the company hits a snag after their accounting system failed. This particularly affected their mutual fund clients who were unable to provide NAVs to potential investors, and thus lost out on business. This issue persisted over two weeks and called into question the reliability of BK's systems.

After recovering in 2016 the stock continued climbing until 2018. The uncertainty about future Fed policy, an inverted yield curve and multiple poor earnings reports led to a drop of 28% from 2018 until the start of the pandemic.

During the pandemic, after a large immediate drop much like the rest of the market, the stock did very well. One reason may be that as financial companies were working remotely the demand for technology solutions (such as trading software) increased dramatically.

The stock struggled in 2022, due to quick rate hikes from the FED to combat inflation. Although not too much of BNY's business is retail banking, they still have large exposure to interest rates and the inability to predict FED policy and economic outcomes hurt much of the financial industry.

Currently the stock is on a strong run, similar to other banks. However an additional wrinkle helps explain BNY's performance. They have commanded investments to AI at a time where the market has rewarded companies for it. They were the first major bank to deploy an "AI Supercomputer" while collaborating with NVIDIA, they already have a history of providing technology solutions to other banks and being the largest custodian they have access to a lot of data.

# Part 1)

Before fitting any ARMA models we should see if the daily log returns are stationary. We can do this with a time series plot.

```{r}
plot(Yn, grid.col = "lightgray", main = "BK Daily Log Returns")
```

In this plot we can see oscillation around 0%, reflecting mean reversion. This indicates that modeling this series as a stationary process is a reasonable approach.

We look for evidence of serial correlation. We can examine the auto-correlations individually to understand their structure.

```{r}
Acf(Yn, ylim = c(-.15,.15))
```

In this plot, test bounds for the null hypothesis that each autocorrelation is zero are drawn. The band is $\pm 1.96/\sqrt{n}$, so in this case $\pm1.96/\sqrt{2892}=0.03644$. There is strong autocorrelation at lag 1 and evidence of autocorrelation (although the effect is quite small) in lags 6,7 and 8.

We can also test for white noise, whether all auto correlations are equal to zero. That is, for the null hypothesis:

$$
H_0: \rho_1=\dots = \rho_K=0
$$

We do this using the Box-Pierce statistic with the Ljung and Box modification which corrects bias and improves the bias for our purposes. The statistic is:

$$
Q(K)=n(n+2) \sum_{l=1}^K\frac{\hat{\rho_l}^2}{n-l}
$$

This approximates the $\chi^2_k$ distribution.

The decision of K is important as it can affect the performance of the statistic, a method to help verify our results is to simply use multiple values of K and compare results.

```{r}
Ks = 2 + 3*(0:5); pv = c()
for(i in Ks) pv = cbind(pv, Box.test(Yn, i , "L")$p.val)
colnames(pv) = paste("K = ", Ks); rownames(pv) = "BK"; round(pv,5)
```

We can soundly reject the null hypothesis that Bank of New York Mellon returns are white noise.

With the knowledge that this series can be modeled as a stationary process and evidence of autocorrelation a reasonable approach would be to use an ARMA model.

An $\text{ARMA}(p,q)$ model is appropriate when a time series $Y_t$ is defined as:

$$
Y_t=c+\phi_1 Y_{t-1}+ \dots+\phi_p+\epsilon_t+\theta_1 \epsilon_{t-1} + \dots +\theta_q \epsilon_{t-q}, \quad c=\mu(1-\phi_1- \dots - \phi_p).
$$

This can also be written as:

$$
\phi(B)x_t=\theta(B) \epsilon_t
$$

Key things we need in order to assure our series is stationary, causal and invertible, is that both the AR and MA polynomials have roots within the unit circle. And there should be no common factors between the two polynomials (if we had common factors it would add error to the model and hurt our ability to use it effectively).

With the concern of parameter redundancy in mind we do not rush to an auto selection tool, we instead start our model building leveraging auto correlations and partial auto correlations.

The auto-correlations can help guide our selection of p and the sample partial auto-correlations can help guide our selection of q.

```{r}
par(mfrow = c(1,2))
Acf(Yn, ylim = c(-.15,.15))
Pacf(Yn, ylim = c(-.15,.15))
```

From these plots the most clear selection of p and q are 1 and 1 respectively. There is evidence of lags 6-8 of ACF and PACF having non-zero correlation but it is unlikely that one, the effect is significant as we are only touching correlations of about 0.05 and two, we are almost certain to run into parameter redundancy with p/q values being that high.

We continue by considering models with a maximum p of 1 and a maximum q of 1, these would be the reasonable set of models.

```{r}
ps = 0:1; qs = 0:1
AIC = BIC = matrix(nrow = length(ps), ncol = length(qs))
rownames(AIC) = rownames(BIC) = paste0("p = ", ps)
colnames(AIC) = colnames(BIC) = paste0("q = ", qs)
for(i in ps){
  for(j in qs){
    if((i+j)<= 3){
      arma = Arima(Yn, order = c(i,0,j))
      AIC[i+1,j+1] = arma$aic; BIC[i+1,j+1] = arma$bic
    }
  }
}
AIC;
BIC;
```

The AIC criterion prefers the candidate models in the following order: AR(1), ARMA(1,1), MA(1), ARMA(0,0).

The BIC criterion prefers the candidate models in the following order: AR(1), MA(1), ARMA(1,1), ARMA(0,0).

Since ARMA(1,1) is a competitive model, we should check for parameter redundancy to see if it can be a good candidate when we introduce ARCH/GARCH effects.

```{r}
arma_1_1 = Arima(Yn, order = c(i,0,j))
plot_roots(coef(arma_1_1))
```

Unfortunately, this model has roots that are pretty close together indicating parameter redundancy. As such, we will remove it from consideration.

After this analysis we have three models in consideration, AR(1), MA(1) and ARMA(0,0) which is white noise. Given that both AIC and BIC agree on the AR(1) model, I feel comfortable deciding on AR(1) as our ARMA model without any further analysis.

```{r}
AR_1 <- Arima(Yn, order = c(1,0,0))
AR_1
```

We can see that our coefficient ar1 is significant here.

# Part 2)

One of the tell tale signs of an ARCH effect is clusters of high variance, indicating that although variance is constant conditional variance is not. We look at the daily log return time series plot again.

```{r}
plot(Yn, grid.col = "lightgray", main = "BK Daily Log Returns")
```

We can clearly see that large movements cluster together, most evident at the start of 2009 and at the start of COVID in March of 2020.

Due to this artifact, our ARMA model will struggle with provide us a prediction interval with suitable coverage in these shock times, it will be too narrow.

Ultimately we are most interested in the prediction invervals during times of shock, this is when our ability to measure risk is very important. This fact motives us to find a model which can improve our coverage rates during periods of high volatility.

We can verify that an ARCH effect in the data is apparent by looking at the ACF of the squared return data:

```{r}

Acf(Yn^2, ylim = c(-.15,.5))
```

There is significance in every ACF, it is clear there is an ARCH effect.

We now proceed to fit an ARMA + GARCH model. To start we will carry the ARMA model from part 1 (AR(1)) forward to this model, and choose GARCH(1,1) due to its simplicity and general effectiveness.

```{r}
spec = ugarchspec(mean.model = list(armaOrder = c(1, 0)),
                  variance.model = list(garchOrder = c(1,1)))
fit = ugarchfit(data = Yn, spec = spec)
showShort(fit)
```

First we examine the significance of the AR(1) parameters. Mu is significant with a p-value of 0.02783. AR1 it is just barely insignificant at a $\alpha =0.05$ threshold having a p-value of 0.057. Due to this we should consider the other candidate models, MA(1) and ARMA(0,0). I will also use AIC and BIC as a way to compare the models, so we can note that AIC is -5.3963 and BIC is -5.386.

```{r}
spec2 = ugarchspec(mean.model = list(armaOrder = c(0, 1)),
                  variance.model = list(garchOrder = c(1,1)))
fit2 = ugarchfit(data = Yn, spec = spec2)
showShort(fit2)
```

In this fit we use MA(1), mu is again significant with a p-value below 0.05. Coefficient MA1 is barely insignificant with a p-value of 0.058948. AIC is -5.3963 and BIC is -5.386.

```{r}
spec3 = ugarchspec(mean.model = list(armaOrder = c(0, 0)),
                  variance.model = list(garchOrder = c(1,1)))
fit3 = ugarchfit(data = Yn, spec = spec3)
showShort(fit3)
```

In this fit we use the ARMA(0,0) model, one corresponding to white noise. We see that the mu coefficient remains significant. AIC is -5.3958, BIC is -5.3875. Lets now compare our three candidate models:

| ARMA Model | AIC     | BIC     |
|------------|---------|---------|
| AR(1)      | -5.3963 | -5.386  |
| MA(1)      | -5.3963 | -5.386  |
| ARMA(0,0)  | -5.3958 | -5.3875 |

We can see that AIC would select either the AR(1) or MA(1) model, they are tied, and BIC would select the ARMA(0,0) model. In this case I would lean on the principle of model parsimony for return data and select the ARMA(0,0) model. Neither the AR1 or MA1 coefficients were statistically significant and BIC, which values a models simplicity more than AIC, selected the ARMA(0,0) model.

However, the ARMA(0,0) + GARCH(1,1) plot model struggles with correlation in the residuals. We can see this as it rejects the null hypothesis of no serial correlation in the Weighted Ljung-Box test. AR(1) + GARCH(1,1) and MA(1) + GARCH(1,1) don't have this issue, which motivates me to continue to dig into other specifications before deciding on the ARMA model that we will use.

So, lets look at the assumption of a normal distribution of errors for each model:

```{r}
par(mfrow = c(1,3))
plot(fit, which = 9, main = "Norm QQ Plot AR(1)")
plot(fit2, which = 9, main = "Norm QQ Plot AR(1)")
plot(fit3, which = 9, main = "Norm QQ Plot AR(1)")
```

This assumption is broken by all three candidate models, lets see if a different distribution would work better. We do this by using QQ plots for all 8 possible distributions:

```{r}
par(mfrow = c(2,3))
#Geting residuals
at1 = residuals(fit)
et1 = residuals(fit, stand = T)

#fitting distribution
dists = c("std", "sstd", "ged", "sged", "nig", "jsu")
fits = vector("list", 6)
for (i in 1:6) fits[[i]] = fitdist(dists[i], et1)

#Plotting QQ Plots
n = length(Yn)
q = ((1:n) - 0.5)/n
qy = quantile(et1, q) # sample quantiles
for(i in 1:length(dists)) {
  ests = fits[[i]]$pars
  qx = qdist(dists[i],p = q_grid, mu = est["mu"], sigma = est["sigma"], skew = est["skew"],
             shape = est["shape"]) ## parametric quantiles
  plot(qx, qy, main = dists[i], ylab = "sample", xlab = "parametric",
       xlim = c(min(et1), -min(et1)), ylim = c(min(et1), -min(et1)))
  abline(lsfit(qx,qy)$coef, col = "red3")
  
}
```

The standardized T and Johnson $S_U$ distributions in particular provide a nice fit for the residuals generated by the AR(1) + GARCH(1,1) model.

```{r}
par(mfrow = c(2,3))
#Geting residuals
at2 = residuals(fit2)
et2 = residuals(fit2, stand = T)

#fitting distribution
dists = c("std", "sstd", "ged", "sged", "nig", "jsu")
fits = vector("list", 6)
for (i in 1:6) fits[[i]] = fitdist(dists[i], at2)

#Plotting QQ Plots
n = length(Yn)
q_grid = (1:n) / (n + 1)
for(i in 1:length(dists)) {
  est = fits[[i]]$pars
  theoretical_quantile <- qdist(distribution = dists[i],p = q_grid, mu = est["mu"],
        sigma = est["sigma"], skew = est["skew"], shape = est["shape"])
  
  qqplot(x = theoretical_quantile, y = as.numeric(at2), main = dists[i],
         xlab = "Theoretical Quantile",ylab= "Sample Quantile")
    abline(lm(quantile(at2, c(0.25, 0.75)) ~ 
             qdist(distribution = dists[i], p = c(0.25, 0.75), mu = est["mu"],
                   sigma = est["sigma"], skew = est["skew"], shape = est["shape"])))
}
```

The standardized T, Johnson $S_U$ and skewed standardized t distributions in particular provide a nice fit for the residuals generated by the MA(1) + GARCH(1,1) model.

```{r}
par(mfrow = c(2,3))
#Geting residuals
at3 = residuals(fit3)
et3 = residuals(fit3, stand = T)

#fitting distribution
dists = c("std", "sstd", "ged", "sged", "nig", "jsu")
fits = vector("list", 6)
for (i in 1:6) fits[[i]] = fitdist(dists[i], at2)

#Plotting QQ Plots
n = length(Yn)
q_grid = (1:n) / (n + 1)
for(i in 1:length(dists)) {
  est = fits[[i]]$pars
  theoretical_quantile <- qdist(distribution = dists[i],p = q_grid, mu = est["mu"],
        sigma = est["sigma"], skew = est["skew"], shape = est["shape"])
  
  qqplot(x = theoretical_quantile, y = as.numeric(at3), main = dists[i],
         xlab = "Theoretical Quantile",ylab= "Sample Quantile")
    abline(lm(quantile(at3, c(0.25, 0.75)) ~ 
             qdist(distribution = dists[i], p = c(0.25, 0.75), mu = est["mu"],
                   sigma = est["sigma"], skew = est["skew"], shape = est["shape"])))
}
```

Similarly to the previous fit, the standardized T, Johnson $S_U$ and skewed standardized t distributions in particular provide a nice fit for the residuals generated by the ARMA(0,0) + GARCH(1,1) model.

We can verify which of these three distributions we should choose for each model, we calculate the AIC and BIC and compare:

```{r}

dists = c("std", "sstd", "jsu")
#Model 1 fits
fits_1 = vector("list", 3)
for (i in 1:3) fits_1[[i]] = fitdist(dists[i], at1)

#Model 2 fits
fits_2 = vector("list", 3)
for (i in 1:3) fits_2[[i]] = fitdist(dists[i], at2)

#Model 3 fits
fits_3 = vector("list", 3)
for (i in 1:3) fits_3[[i]] = fitdist(dists[i], at3)


#Maximum Likelihood, parameters
ml_1 = c();ml_2 = c(); ml_3 = c();p = c();
for(i in 1:length(dists)) {
  ml_1[i] = -tail(fits_1[[i]]$values,1)
  ml_2[i] = -tail(fits_2[[i]]$values,1)
  ml_3[i] = -tail(fits_3[[i]]$values,1)
  p[i] = length(fits[[i]]$pars)
}
#AIC/BIC vectors
aic_1 = -2*ml_1 + 2*p; aic_2 = -2*ml_2 + 2*p; aic_3 = -2*ml_3 + 2*p; names(aic_1) = dists; names(aic_2) = dists; names(aic_3) = dists
bic_1 = -2*ml_1 + log(n)*p;bic_2 = -2*ml_2 + log(n)*p;bic_3 = -2*ml_3 + log(n)*p; names(bic_1) = dists; names(bic_2) = dists; names(bic_3) = dists

cat("Model 1: \n")
rbind(AIC = aic_1, BIC = bic_1)

cat("Model 2: \n")
rbind(AIC = aic_2, BIC = bic_2)

cat("Model 3: \n")
rbind(AIC = aic_3, BIC = bic_3)

```

For all three models JSU is selected by both AIC and BIC. We will use JSU as the distribution of residuals:

```{r}
spec = ugarchspec(mean.model = list(armaOrder = c(1, 0)),
                  variance.model = list(garchOrder = c(1,1)),
                  distribution.model = "jsu")
fit = ugarchfit(data = Yn, spec = spec)
showShort(fit)
```

```{r}
spec2 = ugarchspec(mean.model = list(armaOrder = c(0, 1)),
                  variance.model = list(garchOrder = c(1,1)),
                  distribution.model = "jsu")
fit2 = ugarchfit(data = Yn, spec = spec2)
showShort(fit2)
```

```{r}
spec3 = ugarchspec(mean.model = list(armaOrder = c(0, 0)),
                  variance.model = list(garchOrder = c(1,1)),
                  distribution.model = "jsu")
fit3 = ugarchfit(data = Yn, spec = spec3)
showShort(fit3)
```

Now, a big difference can be seen the AR1 coefficient of the AR(1) + GARCH(1,1) model and the MA1 coefficient of the MA(1) + GARCH(1,1) model are both highly significant. Lets compare the AIC and BIC for the three fits:

| ARMA Model | AIC     | BIC     |
|------------|---------|---------|
| AR(1)      | -5.4983 | -5.4838 |
| MA(1)      | -5.4982 | -5.4838 |
| ARMA(0,0)  | -5.4952 | -5.4828 |

After specifying the distribution of residuals, both the AIC and BIC select the AR(1) + GARCH(1,1) model. Additionally the AR(1) + GARCH(1,1) model does not have correlation in residuals or squared residuals as it fails to reject the null hypothesis in both Weighted Ljung-Box Tests, and ARCH effects are well handled as can be seen by the weighted ARCH LM tests all having high p-values. We finally select AR(1) as our ARMA portion of the ARMA + GARCH model. Before concluding on AR(1) + GARCH(1,1) we should check up to GARCH(2,2) to see if fit is improved.

```{r}
ic = c(); ind = 0
fits = vector("list", 6)
for(p in 1:2) {
  for(q in 0:2){
    ind = ind + 1
    spec = ugarchspec(mean.model = list(armaOrder = c(1,0)),
                      variance.model = list(garchOrder = c(p,q)),
                      distribution.model = "jsu")
    garch = ugarchfit(data = Yn, spec = spec)
    ic = cbind(ic, infocriteria(garch))
    fits[[ind]] = garch
    names(fits)[ind] = paste0("garch", p, q)
  }
}
colnames(ic) = names(fits);ic

cat("Model Selected: "); apply(ic, 1, function(u) colnames(ic)[which.min(u)])
```

All information criteria choose GARCH(1,1) as our GARCH component. I feel confident choosing AR(1) + GARCH(1,1) with Johnson $S_U$ conditional error distribution as the final model for the Bank of New York Mellon daily return series.

Now we need to validate the model with rolling forecasts
